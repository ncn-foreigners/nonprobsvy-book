<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.1.189">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>Modern inference methods for non-probability samples with R - 3&nbsp; Inverse probability weighting</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1.6em;
  vertical-align: middle;
}
</style>


<script src="site_libs/quarto-nav/quarto-nav.js"></script>
<script src="site_libs/quarto-nav/headroom.min.js"></script>
<script src="site_libs/clipboard/clipboard.min.js"></script>
<script src="site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="site_libs/quarto-search/fuse.min.js"></script>
<script src="site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="./">
<link href="./mi.html" rel="next">
<link href="./intro.html" rel="prev">
<script src="site_libs/quarto-html/quarto.js"></script>
<script src="site_libs/quarto-html/popper.min.js"></script>
<script src="site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="site_libs/quarto-html/anchor.min.js"></script>
<link href="site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="site_libs/bootstrap/bootstrap.min.js"></script>
<link href="site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 20,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit"
  }
}</script>



<meta charset="utf-8">
<meta name="generator" content="quarto-1.1.189">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>macros</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1.6em;
  vertical-align: middle;
}
</style>


<script src="macros_files/libs/clipboard/clipboard.min.js"></script>
<script src="macros_files/libs/quarto-html/quarto.js"></script>
<script src="macros_files/libs/quarto-html/popper.min.js"></script>
<script src="macros_files/libs/quarto-html/tippy.umd.min.js"></script>
<script src="macros_files/libs/quarto-html/anchor.min.js"></script>
<link href="macros_files/libs/quarto-html/tippy.css" rel="stylesheet">
<link href="macros_files/libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="macros_files/libs/bootstrap/bootstrap.min.js"></script>
<link href="macros_files/libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="macros_files/libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">

  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

</head>

<body class="fullcontent nav-sidebar floating">

<div id="quarto-content" class="page-columns page-rows-contents page-layout-article">

<main class="content" id="quarto-document-content">



<div class="hidden">
<p><span class="math display">\[
\newcommand{\bSigma}{\boldsymbol{\Sigma}}
\newcommand{\bOmega}{\boldsymbol{\Omega}}
\newcommand{\bTheta}{\boldsymbol{\Theta}}
\newcommand{\bPi}{\boldsymbol{\Pi}}
\newcommand{\bbeta}{\boldsymbol{\beta}}
\newcommand{\balpha}{\boldsymbol{\alpha}}
\newcommand{\brho}{\boldsymbol{\rho}}
\newcommand{\beps}{\boldsymbol{\epsilon}}
\newcommand{\blambda}{\boldsymbol{\lambda}}
\newcommand{\bgamma}{\boldsymbol{\gamma}}
\newcommand{\btheta}{\boldsymbol{\theta}}
\newcommand{\bmu}{\boldsymbol{\mu}}
\newcommand{\bpi}{\boldsymbol{\pi}}
\newcommand{\bphi}{\boldsymbol{\phi}}
\newcommand{\bPhi}{\boldsymbol{\Phi}}
\newcommand{\boldeta}{\boldsymbol{\eta}}
\newcommand{\bx}{\boldsymbol{x}}
\newcommand{\bD}{\boldsymbol{D}}
\newcommand{\bV}{\boldsymbol{V}}
\newcommand{\bv}{\boldsymbol{v}}
\newcommand{\bY}{\boldsymbol{Y}}
\newcommand{\bA}{\boldsymbol{A}}
\newcommand{\bB}{\boldsymbol{B}}
\newcommand{\bR}{\boldsymbol{R}}
\newcommand{\bM}{\boldsymbol{M}}
\newcommand{\bI}{\boldsymbol{I}}
\newcommand{\bC}{\boldsymbol{C}}
\newcommand{\bW}{\boldsymbol{W}}
\newcommand{\bw}{\boldsymbol{w}}
\newcommand{\bd}{\boldsymbol{d}}
\newcommand{\bT}{\boldsymbol{T}}
\newcommand{\bt}{\boldsymbol{t}}
\newcommand{\bZ}{\boldsymbol{Z}}
\newcommand{\bX}{\boldsymbol{X}}
\newcommand{\bz}{\boldsymbol{z}}
\newcommand{\by}{\boldsymbol{y}}
\newcommand{\br}{\boldsymbol{r}}
\newcommand{\bp}{\boldsymbol{p}}
\newcommand{\bb}{\boldsymbol{b}}
\newcommand{\bZero}{\boldsymbol{0}}
\newcommand{\bOne}{\boldsymbol{1}}
\]</span></p>
</div>

</main>
<!-- /main column -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    target: function(trigger) {
      return trigger.previousElementSibling;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    setTimeout(function() {
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  function tippyHover(el, contentFn) {
    const config = {
      allowHTML: true,
      content: contentFn,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start'
    };
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const cites = ref.parentNode.getAttribute('data-cites').split(' ');
    tippyHover(ref, function() {
      var popup = window.document.createElement('div');
      cites.forEach(function(cite) {
        var citeDiv = window.document.createElement('div');
        citeDiv.classList.add('hanging-indent');
        citeDiv.classList.add('csl-entry');
        var biblioDiv = window.document.getElementById('ref-' + cite);
        if (biblioDiv) {
          citeDiv.innerHTML = biblioDiv.innerHTML;
        }
        popup.appendChild(citeDiv);
      });
      return popup.innerHTML;
    });
  }
});
</script>
</div> <!-- /content -->





  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>





<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
  <nav class="quarto-secondary-nav" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
    <div class="container-fluid d-flex justify-content-between">
      <h1 class="quarto-secondary-nav-title"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">Inverse probability weighting</span></h1>
      <button type="button" class="quarto-btn-toggle btn" aria-label="Show secondary navigation">
        <i class="bi bi-chevron-right"></i>
      </button>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse sidebar-navigation floating overflow-auto">
    <div class="pt-lg-2 mt-2 text-left sidebar-header">
    <div class="sidebar-title mb-0 py-0">
      <a href="./">Modern inference methods for non-probability samples with R</a> 
        <div class="sidebar-tools-main">
    <a href="https://github.com/ncn-foreigners/nonprobsvy-book" title="Source Code" class="sidebar-tool px-1"><i class="bi bi-github"></i></a>
    <a href="" title="Download" id="sidebar-tool-dropdown-0" class="sidebar-tool dropdown-toggle px-1" data-bs-toggle="dropdown" aria-expanded="false"><i class="bi bi-download"></i></a>
    <ul class="dropdown-menu" aria-labelledby="sidebar-tool-dropdown-0">
        <li>
          <a class="dropdown-item sidebar-tools-main-item" href="./Modern-inference-methods-for-non-probability-samples-with-R.pdf">
            <i class="bi bi-bi-file-pdf pe-1"></i>
          Download PDF
          </a>
        </li>
        <li>
          <a class="dropdown-item sidebar-tools-main-item" href="./Modern-inference-methods-for-non-probability-samples-with-R.epub">
            <i class="bi bi-bi-journal pe-1"></i>
          Download ePub
          </a>
        </li>
    </ul>
</div>
    </div>
      </div>
      <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Search"></div>
        </div>
      </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./index.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">1</span>&nbsp; <span class="chapter-title">Welcome!</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./intro.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">Introduction and Overview</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./ipw.html" class="sidebar-item-text sidebar-link active"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">Inverse probability weighting</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./mi.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">Mass imputation</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./dr.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">5</span>&nbsp; <span class="chapter-title">Doubly robust methods</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./variableselection.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">6</span>&nbsp; <span class="chapter-title">Techniques of variables selection for high-dimensional data</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./summary.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">7</span>&nbsp; <span class="chapter-title">Summary</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./references.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">8</span>&nbsp; <span class="chapter-title">References</span></a>
  </div>
</li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" aria-expanded="true">Appendices</a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" aria-expanded="true">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-1" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./appendix.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">A</span>&nbsp; <span class="chapter-title">Appendices</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./nomenclature.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">B</span>&nbsp; <span class="chapter-title">Nomenclature</span></a>
  </div>
</li>
      </ul>
  </li>
    </ul>
    </div>
</nav>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">Table of contents</h2>
   
  <ul>
  <li><a href="#motivation-and-assumptions" id="toc-motivation-and-assumptions" class="nav-link active" data-scroll-target="#motivation-and-assumptions"><span class="toc-section-number">3.1</span>  Motivation and assumptions</a></li>
  <li><a href="#maximum-likelihood-estimation" id="toc-maximum-likelihood-estimation" class="nav-link" data-scroll-target="#maximum-likelihood-estimation"><span class="toc-section-number">3.2</span>  Maximum likelihood estimation</a>
  <ul class="collapse">
  <li><a href="#logistic-regression" id="toc-logistic-regression" class="nav-link" data-scroll-target="#logistic-regression"><span class="toc-section-number">3.2.1</span>  Logistic regression</a></li>
  <li><a href="#complementary-log-log-regression" id="toc-complementary-log-log-regression" class="nav-link" data-scroll-target="#complementary-log-log-regression"><span class="toc-section-number">3.2.2</span>  Complementary log-log regression</a></li>
  <li><a href="#probit-regression" id="toc-probit-regression" class="nav-link" data-scroll-target="#probit-regression"><span class="toc-section-number">3.2.3</span>  Probit regression</a></li>
  </ul></li>
  <li><a href="#general-estimating-equations" id="toc-general-estimating-equations" class="nav-link" data-scroll-target="#general-estimating-equations"><span class="toc-section-number">3.3</span>  General estimating equations</a>
  <ul class="collapse">
  <li><a href="#logistic-regression-1" id="toc-logistic-regression-1" class="nav-link" data-scroll-target="#logistic-regression-1"><span class="toc-section-number">3.3.1</span>  Logistic regression</a></li>
  <li><a href="#complementary-log-log-regression-1" id="toc-complementary-log-log-regression-1" class="nav-link" data-scroll-target="#complementary-log-log-regression-1"><span class="toc-section-number">3.3.2</span>  Complementary log-log regression</a></li>
  <li><a href="#probit-regression-1" id="toc-probit-regression-1" class="nav-link" data-scroll-target="#probit-regression-1"><span class="toc-section-number">3.3.3</span>  Probit regression</a></li>
  </ul></li>
  <li><a href="#population-mean-estimator-and-its-properties" id="toc-population-mean-estimator-and-its-properties" class="nav-link" data-scroll-target="#population-mean-estimator-and-its-properties"><span class="toc-section-number">3.4</span>  Population mean estimator and its properties</a>
  <ul class="collapse">
  <li><a href="#variance-of-an-estimator" id="toc-variance-of-an-estimator" class="nav-link" data-scroll-target="#variance-of-an-estimator"><span class="toc-section-number">3.4.1</span>  Variance of an estimator</a></li>
  </ul></li>
  </ul>
<div class="toc-actions"><div><i class="bi bi-github"></i></div><div class="action-links"><p><a href="https://github.com/ncn-foreigners/nonprobsvy-book/issues/new" class="toc-action">Report an issue</a></p></div></div></nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title d-none d-lg-block"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">Inverse probability weighting</span></h1>
</div>



<div class="quarto-title-meta">

    
    
  </div>
  

</header>

<section id="motivation-and-assumptions" class="level2" data-number="3.1">
<h2 data-number="3.1" data-anchor-id="motivation-and-assumptions"><span class="header-section-number">3.1</span> Motivation and assumptions</h2>
<p>The biggest drawback of the nonprobability sampling is unknown selection mechanism for a unit to be included in the sample. This is why we talk about so called “biased sample” problem. Inverse probabiliy approach is based on assumption that reference probabiliy sample (<span class="math inline">\(S_B\)</span>) is available and therefore we can estimate propensity score of selection mechanism.</p>
<p>Let <span class="math inline">\(\mathcal{U}=\{1,2, \ldots, N\}\)</span> represent the finite population with N units and <span class="math inline">\(\left\{\left(\bx_i, y_i\right), i \in \mathcal{S}_{\mathrm{A}}\right\}\)</span> and <span class="math inline">\(\{\left(\bx_i, d_i^B), i \in \mathcal{S}_{\mathrm{B}}\right\}\)</span> be the datasets from non-probability and probability samples respectively and let <span class="math inline">\(\bY = \left(Y_1, Y_2, \cdots, Y_{n_{A}}\right)^{T}\)</span> and let <span class="math inline">\(\bX_A\)</span>, <span class="math inline">\(\bX_B\)</span> denote an <span class="math inline">\(n \times \left( p+1 \right)\)</span> matrices of regression coefficients for samples <span class="math inline">\(S_A\)</span> and <span class="math inline">\(S_B\)</span> of the form <span class="math display">\[
\begin{equation*}
    \bX_A =
    \begin{bmatrix}
        1 &amp; x_{11} &amp; x_{12} &amp; \cdots &amp; x_{1p} \\
        1 &amp; x_{21} &amp; x_{22} &amp; \cdots &amp; x_{2p} \\
        \vdots &amp; \vdots &amp; \ddots &amp; \vdots \\
        1 &amp;  x_{n_{A}1} &amp; x_{n_{A}2} &amp; \cdots &amp; x_{n_{A}p} \\
    \end{bmatrix}
\end{equation*}
\]</span><br>
<span class="math display">\[
\begin{equation*}
    \bX_B =
    \begin{bmatrix}
        1 &amp; x_{11} &amp; x_{12} &amp; \cdots &amp; x_{1p} \\
        1 &amp; x_{21} &amp; x_{22} &amp; \cdots &amp; x_{2p} \\
         \vdots &amp; \vdots &amp; \ddots &amp; \vdots \\
        1 &amp; x_{n_{B}1} &amp; x_{n_{B}2} &amp; \cdots &amp; x_{n_{B}p} \\
    \end{bmatrix}
\end{equation*}
\\
\]</span></p>
<p>Following assumptions are required for this model:</p>
<ol type="1">
<li><p>The selection indicator <span class="math inline">\(R_i\)</span> and the response variable <span class="math inline">\(y_i\)</span> are independent given the set of covariates <span class="math inline">\(\bx_i\)</span>.</p></li>
<li><p>All units have a nonzero propensity score, that is, <span class="math inline">\(\pi_i^A &gt; 0\)</span> for all <span class="math inline">\(i\)</span>.</p></li>
<li><p>The indicator variables <span class="math inline">\(R_i^A\)</span> and <span class="math inline">\(R_j^A\)</span> are independent for given <span class="math inline">\(\bx_i\)</span> and <span class="math inline">\(\bx_j\)</span> for <span class="math inline">\(i \neq j\)</span>.</p></li>
</ol>
<p>There are few methods for inclusion probability estimation for <span class="math inline">\(S_A\)</span>. The first one is based on Maximum Likelihood approach but with correction for access to a random sample rather than the entire population. On can also construct calibration equation, so that estimated propensity weights allow to reproduce the whole population. Let us to perform technical details of these methods.</p>
</section>
<section id="maximum-likelihood-estimation" class="level2" data-number="3.2">
<h2 data-number="3.2" data-anchor-id="maximum-likelihood-estimation"><span class="header-section-number">3.2</span> Maximum likelihood estimation</h2>
<p>Suppose that propensity score can be modelled parametrically as <span class="math inline">\(\mathbb{P}\left(R_i=1 \mid \bx_i\right) = \pi(\bx_{i}, \btheta_{0})\)</span>. The maximum likelihood estimator is computed as <span class="math inline">\(\hat{\pi}_{i}^{A} = \pi(\bx_{i}, \hat{\btheta}_{0})\)</span>, where <span class="math inline">\(\hat{\btheta}_{0}\)</span> is the maximizer of the following log-likelihood function:</p>
<p><span class="math display">\[
    \begin{split}
\ell(\boldsymbol{\theta}) &amp; =\sum_{i=1}^N\left\{R_i \log \pi_i^{\mathrm{A}}+\left(1-R_i\right) \log \left(1-\pi_i^{\mathrm{A}}\right)\right\} \\ &amp; =\sum_{i \in \mathcal{S}_{\mathrm{A}}} \log \left\{\frac{\pi\left(\boldsymbol{x}_i, \boldsymbol{\theta}\right)}{1-\pi\left(\boldsymbol{x}_i, \boldsymbol{\theta}\right)}\right\}+\sum_{i=1}^N \log \left\{1-\pi\left(\boldsymbol{x}_i, \boldsymbol{\theta}\right)\right\}
    \end{split}
\]</span></p>
<p>Since we do not observe <span class="math inline">\(\bx_i\)</span> for all units, Yilin Chen, Pengfei Li &amp; Changbao Wu presented following log-likelihood function is subject to data integration basing on samples <span class="math inline">\(S_A\)</span> and <span class="math inline">\(S_B\)</span>. They proposed logistic regression model with <span class="math inline">\(\pi(\bx_{i}, \btheta) = \frac{\exp(\bx_{i}^{\top}\btheta)}{\exp(\bx_{i}^{\top}\btheta) + 1}\)</span> in order to estimate <span class="math inline">\(\btheta\)</span>. We expanded this approach on probit regression and complementary log-log model. For the sake of accuracy, let us recall that the probit and cloglog models are based on the assumption that model takes the form <span class="math inline">\(\pi(\bx_{i},\btheta) = \Phi(\bx_{i}^{\top}\btheta)\)</span> and <span class="math inline">\(\pi(\bx_{i}, \btheta) = 1 - \exp(-\exp(\bx_{i}^{\top}\btheta))\)</span> respectively.</p>
<p><span class="math display">\[
    \ell^{*}(\btheta) = \sum_{i \in S_{A}}\log \left\{\frac{\pi(\bx_{i}, \btheta)}{1 - \pi(\bx_{i},\btheta)}\right\} + \sum_{i \in S_{B}}d_{i}^{B}\log\{1 - \pi({\bx_{i},\btheta})\}
\]</span> In order to maximize function from (3.1) equate its derivative (gradient) to zero. This will give us p+1 nonlinear equations with respect to <span class="math inline">\(\btheta\)</span>, which will have no explicit solutions. To solve the given system of equations, the Newton-Raphson algorithm can be used. This method also requires the calculation of the second derivative of the log-likelihood function (hessian). By expanding the function (3.1) into a Taylor series, it can be shown that <span class="math inline">\(\hat{\btheta}\)</span> can be found by using the following iterative method: <span class="math display">\[
\btheta^{(m)} = \btheta^{(m-1)} - \{H(\btheta^{(m-1)}\}^{-1}U(\btheta^{(m-1})),
\]</span> where <span class="math inline">\(\operatorname{H}\)</span> - hessian, <span class="math inline">\(\operatorname{U}\)</span> - gradient. This will give us a convergent MLE estimator (for <span class="math inline">\(m \rightarrow \infty\)</span>).</p>
<p>In the <code>nonprobsvy</code> package, in addition to the Newton-Raphson method, you can also use the Nelder-Mead and Broyden-Fletcher-Goldfarb-Shanno methods implemented in the <code>optim</code> and <code>maxLik</code> functions.</p>
<p>From the chain rule for counting derivatives, we know that <span class="math display">\[
\begin{equation*}
    \frac{\partial \ell}{\partial \boldsymbol{\theta}}=\frac{\partial \ell}{\partial p} \frac{\partial p}{\partial \eta} \frac{\partial \eta}{\partial \boldsymbol{\theta}}=\frac{\partial \ell}{\partial p} \frac{\partial p}{\partial \eta} \boldsymbol{X}
\end{equation*}
\]</span> where <span class="math inline">\(\eta=\boldsymbol{X}^{\top} \boldsymbol{\theta}\)</span>, <span class="math inline">\(\frac{\partial \ell}{\partial \boldsymbol{\theta}}=\boldsymbol{U} \boldsymbol{X}\)</span> (<span class="math inline">\(\boldsymbol{U} = \frac{\partial \ell}{\partial {\eta}}\)</span>) and <span class="math inline">\(\frac{\partial^2 \ell}{\partial \boldsymbol{\theta} \partial \boldsymbol{\theta}^{\top}}= \boldsymbol{X}^{\top} \boldsymbol{W} \boldsymbol{X}\)</span> (<span class="math inline">\(\boldsymbol{W} = \frac{\partial^2 \ell}{\partial {\eta}}\)</span>). Using this rule In the following subsections we present the full derivation of the MLE n the following subsections. Calculations include technical differences depending on the assumed model for the propensity score.</p>
<section id="logistic-regression" class="level3" data-number="3.2.1">
<h3 data-number="3.2.1" data-anchor-id="logistic-regression"><span class="header-section-number">3.2.1</span> Logistic regression</h3>
<p>Log-likelihood function with logistic regression is given by <span class="math display">\[
\ell^{*}(\btheta) = \sum_{i \in S_A}\bx_{i}^{\top}\btheta - \sum_{i \in S_B}d_{i}^{B}\log\{1 + \exp(\bx_{i}^{\top}\btheta)\}
\]</span> with analytical gradient and hessian given by <span class="math display">\[
\frac{\partial \ell^*}{\partial\btheta} = \sum_{i \in S_{A}}\bx_{i} - \sum_{i \in S_{B}}d_{i}^{B}\pi(\bx_{i}, \btheta)\bx_{i}
\]</span> and <span class="math display">\[
    \frac{\partial^{2} \ell^{*}}{\partial\btheta^{T} \partial\btheta} =- \sum_{i \in S_B}d_i^B\pi(\bx_i,\btheta)(1 - \pi(\bx_i,\btheta))\bx_i\bx_i^{\top} = \bX_B^{\top}\operatorname{\bW}_{B}\bX_B,
\]</span> respectively, where <span class="math display">\[
\begin{align*}
    \operatorname{\bW}_{B} =
    diag &amp; \left(-d_1^B\pi(\bx_{1},\btheta)(1 - \pi(\bx_{1},\btheta)), -d_2^B\pi(\bx_{2},\btheta)(1 - \pi(\bx_{2},\btheta)), \right. \\
     &amp; \left. \ldots, -d_{n_{B}}^{B}\pi(\bx_{n_{B}},\btheta)(1 - \pi(x_{n_{B}},\btheta))\right).
\end{align*}
\]</span></p>
</section>
<section id="complementary-log-log-regression" class="level3" data-number="3.2.2">
<h3 data-number="3.2.2" data-anchor-id="complementary-log-log-regression"><span class="header-section-number">3.2.2</span> Complementary log-log regression</h3>
<p>Similarly, log-likelihood function has form of <span class="math display">\[
\ell^{*}(\btheta) = \sum_{i \in S_{A}}\left\{\log\{1 - \exp(-\exp(\bx_{i}^{\top}\btheta))\} + \exp(\bx_{i}^{\top}\btheta)\right\} - \sum_{i \in S_{B}} d_{i}^{B}\exp(\bx_{i}^{\top}\btheta)
\]</span> with analytical gradient and hessian equal to <span class="math display">\[
    \frac{\partial \ell^*}{\partial\btheta} = \sum_{i \in S_{A}}\frac{\exp(\bx_{i}^{\top}\btheta)\bx_{i}}{\pi(\bx_{i}, \btheta)} - \sum_{i \in S_{B}}d_{i}^{B}\exp(\bx_{i}^{T}\btheta)\bx_{i}
\]</span> and <span class="math display">\[
    \begin{split}
    \frac{\partial^{2} \ell^{*}}{\partial\btheta^{T} \partial\btheta} &amp; = \sum_{i \in S_A} \frac{\exp(\bx_{i}^{\top}\btheta)}{\pi(\bx_{i}, \btheta)} \left\{1 - \frac{\exp(\bx_{i}^{\top}\btheta)}{\pi(\bx_{i}, \btheta)} + \exp(\bx_{i}^{\top}\btheta)\right\}\bx_i\bx_i^{\top} - \sum_{i \in S_B}d_i^B\exp (\bx_{i}^{\top}\btheta)\bx_i\bx_i^{\top} \\ &amp; = \bX_A^{\top}\operatorname{\bW}_{Ac}\bX_A - \bX_B^{\top}\operatorname{\bW}_{Bc}\bX_B,
    \end{split}
\]</span> respectively, where <span class="math display">\[
\begin{align*}
    \operatorname{\bW}_{Ac} =  Diag &amp; \left(\frac{\exp(\bx_{1}^{\top}\btheta)}{\pi(\bx_{1}, \btheta)} \left\{1 - \frac{\exp(\bx_{1}^{\top}\btheta)}{\pi(\bx_{1}, \btheta)} + \exp(\bx_{1}^{\top}\btheta)\right\}, \right.
    \\
    &amp; \left. \frac{\exp(\bx_{2}^{\top}\btheta)}{\pi(\bx_{2}, \btheta)} \left\{1 - \frac{\exp(\bx_{2}^{\top}\btheta)}{\pi(\bx_{2}, \btheta)} + \exp(\bx_{2}^{\top}\btheta)\right\}, \right.
    \\
    &amp; \left. \ldots, \right.
    \\
    &amp; \left. \frac{\exp(\bx_{n_A}^{\top}\btheta)} {\pi(\bx_{n_A}, \btheta)} \left\{1 - \frac{\exp(\bx_{n_A}^{\top}\btheta)}{\pi(\bx_{n_A}, \btheta)} + \exp(\bx_{n_A}^{\top}\btheta)\right\} \right)
\end{align*}
\]</span> and <span class="math display">\[
\begin{align*}
    \operatorname{\bW}_{Bc} = Diag \left(d_1^B\exp (\bx_{1}^{\top}\btheta), d_2^B\exp (\bx_{2}^{\top}\btheta), \ldots, d_{n_B}^B\exp (\bx_{n_{B}}^{\top}\btheta)\right).
\end{align*}
\]</span></p>
</section>
<section id="probit-regression" class="level3" data-number="3.2.3">
<h3 data-number="3.2.3" data-anchor-id="probit-regression"><span class="header-section-number">3.2.3</span> Probit regression</h3>
<p>For probit model calculations are as follow <span class="math display">\[
\begin{align*}
    \begin{split}
        \ell^{*}(\btheta) &amp; = \sum_{i \in S_{A}}\log\left\{\frac{\Phi(\bx_{i}^{\top}\btheta)}{1 - \Phi(\bx_{i}^{\top}\btheta)}\right\} + \sum_{i \in S_{B}}d_{i}^{B}\log\{1 - \Phi(\bx_{i}^{\top}\btheta)\}
    \end{split}
\end{align*}
\]</span> with analytical gradient as <span class="math display">\[
        \frac{\partial \ell^*}{\partial\btheta} = \sum_{i \in S_A}\frac{\phi(\bx_i^{\top}\btheta)}{\Phi(\bx_i^{\top}\btheta)(1 - \Phi(\bx_i^{\top}\btheta))}\bx_i - \sum_{i \in S_B}d_i^B\frac{\phi(\bx_i^{\top}\btheta)}{1 - \Phi(\bx_i^{\top}\btheta)}\bx_i.
\]</span> and hessian equals to <span class="math display">\[
\begin{align*}
    \begin{split}
\frac{\partial^{2} \ell^{*}}{\partial\btheta^{T} \partial\btheta} &amp; = \sum_{i \in S_A}\frac{ - \bx_i^{\top} \phi(\bx_i^{\top}\btheta)}{\Phi(\bx_i^{\top}\btheta)(1 - \Phi(\bx_i^{\top}\btheta) )}\bx_i\bx_i^{\top} - \sum_{i \in S_A}\frac{\phi(\bx_i^{\top}\btheta))^{2} \left(1 - 2\Phi(\bx_i^{\top}\btheta))\right)}{\Phi(\bx_i^{\top}\btheta)^{2}(1 - \Phi(\bx_i^{\top}\btheta) )^{2}}\bx_i\bx_i^{\top} \\ &amp; - \sum_{i \in S_B}\frac{\bx_i^{\top}\btheta \phi(\bx_i^{\top}\btheta)}{(1 -  \Phi(\bx_i^{\top}\btheta))}\bx_i\bx_i^{\top} - \sum_{i \in S_B} \frac{\phi(\bx_i^{\top}\btheta)^{2}}{\left(1 - \Phi(\bx_i^{\top}\btheta) \right)} \\ &amp;=     \bX_A^{\top}\operatorname{\bW}_{Ap}\bX_A - \bX_B^{\top}\operatorname{\bW}_{Bp}\bX_B
    \end{split}
\end{align*}
\]</span> where <span class="math display">\[
\begin{align}
\operatorname{\bW}_{Ap} =  Diag &amp; \left(\frac{ - \bx_1^{\top} \phi(\bx_1^{\top}\btheta)}{\Phi(\bx_1^{\top}\btheta)(1 - \Phi(\bx_1^{\top}\btheta) )}\bx_1\bx_1^{\top} - \frac{\phi(\bx_1^{\top}\btheta))^{2} \left(1 - 2\Phi(\bx_1^{\top}\btheta))\right)}{\Phi(\bx_1^{\top}\btheta)^{2}(1 - \Phi(\bx_1^{\top}\btheta) )^{2}}\bx_1\bx_1^{\top} \right.,
    \\
    &amp; \left. \frac{ - \bx_2^{\top} \phi(\bx_2^{\top}\btheta)}{\Phi(\bx_2^{\top}\btheta)(1 - \Phi(\bx_2^{\top}\btheta) )}\bx_2\bx_2^{\top} - \frac{\phi(\bx_2^{\top}\btheta))^{2} \left(1 - 2\Phi(\bx_2^{\top}\btheta))\right)}{\Phi(\bx_2^{\top}\btheta)^{2}(1 - \Phi(\bx_2^{\top}\btheta) )^{2}}\bx_2\bx_2^{\top}, \right.
    \\
    &amp; \left. \ldots, \right.
    \\
    &amp; \left. \frac{ - \bx_{n_A}^{\top} \phi(\bx_{n_A}^{\top}\btheta)}{\Phi(\bx_{n_A}^{\top}\btheta)(1 - \Phi(\bx_{n_A}^{\top}\btheta) )}\bx_{n_A}\bx_{n_A}^{\top} - \frac{\phi(\bx_{n_A}^{\top}\btheta))^{2} \left(1 - 2\Phi(\bx_{n_A}^{\top}\btheta))\right)}{\Phi(\bx_{n_A}^{\top}\btheta)^{2}(1 - \Phi(\bx_{n_A}^{\top}\btheta) )^{2}}\bx_{n_A}\bx_{n_A}^{\top}\right)
\end{align}
\]</span> and <span class="math display">\[
\begin{align}
\operatorname{\bW}_{Bp} = Diag &amp; \left(\frac{\bx_1^{\top}\btheta \phi(\bx_1^{\top}\btheta)}{(1 -  \Phi(\bx_1^{\top}\btheta))}\bx_1\bx_1^{\top} - \frac{\phi(\bx_1^{\top}\btheta)^{2}}{\left(1 - \Phi(\bx_1^{\top}\btheta) \right)}, \right.
    \\
    &amp;
    \left. \frac{\bx_2^{\top}\btheta \phi(\bx_2^{\top}\btheta)}{(1 -  \Phi(\bx_2^{\top}\btheta))}\bx_2\bx_2^{\top} - \frac{\phi(\bx_2^{\top}\btheta)^{2}}{\left(1 - \Phi(\bx_2^{\top}\btheta) \right)}, \right.
    \\
    &amp; \left. \ldots, \right.
    \\
    &amp; \left. \frac{\bx_{N_B}^{\top}\btheta \phi(\bx_{N_B}^{\top}\btheta)}{(1 -  \Phi(\bx_{N_B}^{\top}\btheta))}\bx_{N_B}\bx_{N_B}^{\top} - \frac{\phi(\bx_{N_B}^{\top}\btheta)^{2}}{\left(1 - \Phi(\bx_{N_B}^{\top}\btheta) \right)}\right)
\end{align}
\]</span></p>
</section>
</section>
<section id="general-estimating-equations" class="level2" data-number="3.3">
<h2 data-number="3.3" data-anchor-id="general-estimating-equations"><span class="header-section-number">3.3</span> General estimating equations</h2>
<p>The pseudo score equations derived from Maximum Likelihood Estimation methods may be replaced by a system of general estimating equations. Let <span class="math inline">\(\operatorname{h}\left(\bx\right)\)</span> be the smooth function and <span class="math display">\[
\begin{equation}
\mathbf{U}(\btheta)=\sum_{i \in S_A} \mathbf{h}\left(\mathbf{x}_i, \btheta\right)-\sum_{i \in S_B} d_i^B \pi\left(\mathbf{x}_i, \btheta\right) \mathbf{h}\left(\mathbf{x}_i, \btheta\right).
\end{equation}
\]</span> Under <span class="math inline">\(\operatorname{h}\left(\bx_i\right) = \bx_i\)</span> and logistic model for propensity score, Equation (2.1) looks like disorted version of the score equation from MLE method. Then <span class="math display">\[
    \mathbf{U}(\btheta)=\sum_{i \in S_A} \bx_i -\sum_{i \in S_B} d_i^B \pi\left(\mathbf{x}_i, \btheta\right) \bx_i.
\]</span> and analytical Jacobian is given by <span class="math display">\[
\frac{\partial \mathbf{U}}{\partial\btheta} = - \sum_{i \in S_B} d_i^B \pi_i^A\left(\bx_i^{\mathrm{T}} \btheta \right) \left(1 -  \pi_i^A\left(\bx_i^{\mathrm{T}} \btheta \right)\right)\bx_i \bx_i^{\mathrm{T}}.
\]</span> The second proposed of the smooth function in the literature is <span class="math inline">\(\operatorname{h}\left(\bx_i\right) = \bx_i / \pi_i^A\left(\bx_i^{\mathrm{T}} \btheta \right)\)</span>, for which the <span class="math inline">\(\operatorname{U}\)</span>-function takes the following form <span class="math display">\[
    \mathbf{U}(\btheta)=\sum_{i \in S_A}  \bx_i \pi_i^A\left(\bx_i^{\mathrm{T}} \btheta \right)^{-1} -\sum_{i \in S_B} d_i^B \bx_i.
\]</span> The most important difference between these two version, however, is the fact that for <span class="math inline">\(\operatorname{h}\left(\bx_i\right) = \bx_i / \pi_i^A\left(\bx_i^{\mathrm{T}} \btheta \right)\)</span>, <span class="math inline">\(\mathbf{U}(\btheta)\)</span> requires only the accessed or estimated population totals for auxiliary variables <span class="math inline">\(\bx\)</span>. This approach can be particularly useful when the values of <span class="math inline">\(\bx\)</span> at the unit for a probability sample are not available. Generally, the goal is to find solution for following system of equations <span class="math display">\[
\begin{equation*}
    \sum_{i \in S_A} \mathbf{h}\left(\mathbf{x}_i, \btheta\right) = \sum_{i \in S_B} d_i^B \pi\left(\mathbf{x}_i, \btheta\right) \mathbf{h}\left(\mathbf{x}_i, \btheta\right)
\end{equation*}
\]</span></p>
<p>Derived <span class="math inline">\(\hat{\btheta}\)</span> estimated from this model can be less efficient than the one based on MLE approach, moreover, limited empirical results show that the solution of <span class="math inline">\(\mathbf{U}(\btheta) = \bZero\)</span> can be unstable for given <span class="math inline">\(\operatorname{h}\left(\bx_i\right)\)</span>. On the other hand estimating equations based methods extend the propensity model to more restrictive estimation conditions, for example, when a vector of population totals/means is available instead of a sample <span class="math inline">\(S_B\)</span>.</p>
<p>In total, we have six models for this estimation method depending on the <span class="math inline">\(\operatorname{h}\)</span>-function and the way propensity score is parameterized. Let us present all of them.</p>
<section id="logistic-regression-1" class="level3" data-number="3.3.1">
<h3 data-number="3.3.1" data-anchor-id="logistic-regression-1"><span class="header-section-number">3.3.1</span> Logistic regression</h3>
<p>As the one model for logistic regression is presented above, we have equation under <span class="math inline">\(\operatorname{h}\left(\bx_i\right) = \bx_i \pi_i^A\left(\bx_i^{\mathrm{T}} \btheta \right)^{-1}\)</span> to consider. Analytical jacobian is given by <span class="math display">\[
    \frac{\partial \operatorname{U}(\btheta)}{\partial \btheta} = -\sum_{i \in S_A} \frac{1 - \pi_i^A\left(\bx_i^{\mathrm{T}} \btheta \right)}{\pi_i^A\left(\bx_i^{\mathrm{T}} \btheta \right)} \bx_i \bx_i^{\mathrm{T}}.
\]</span></p>
</section>
<section id="complementary-log-log-regression-1" class="level3" data-number="3.3.2">
<h3 data-number="3.3.2" data-anchor-id="complementary-log-log-regression-1"><span class="header-section-number">3.3.2</span> Complementary log-log regression</h3>
<p>For <span class="math inline">\(\operatorname{h}\left(\bx_i\right) = \bx_i \pi_i^A\left(\bx_i^{\mathrm{T}} \btheta \right)^{-1}\)</span> analytical jacobian is given by <span class="math display">\[
    \frac{\partial \operatorname{U}(\btheta)}{\partial \btheta} = - \sum_{i \in S_A} \frac{1 - \pi_i^A\left(\bx_i^{\mathrm{T}} \btheta \right)}{\pi_i^A\left(\bx_i^{\mathrm{T}} \btheta \right)^2} \exp(\bx_i^{\mathrm{T}} \btheta) \bx_i \bx_i^{\mathrm{T}}
\]</span> and <span class="math inline">\(\operatorname{h}\left(\bx_i\right) = \bx_i\)</span> we have <span class="math display">\[
    \frac{\partial \operatorname{U}(\btheta)}{\partial \btheta} = - \sum_{i \in S_B} \frac{1 - \pi_i^A\left(\bx_i^{\mathrm{T}} \btheta \right)}{\pi_i^B} \exp \left(\bx_i^\mathrm{T} \btheta\right) \bx_i \bx_i^{\mathrm{T}}.
\]</span></p>
</section>
<section id="probit-regression-1" class="level3" data-number="3.3.3">
<h3 data-number="3.3.3" data-anchor-id="probit-regression-1"><span class="header-section-number">3.3.3</span> Probit regression</h3>
<p>Similarly, for the probit model, under <span class="math inline">\(\operatorname{h}\left(\bx_i\right) = \bx_i \pi_i^A\left(\bx_i^{\mathrm{T}} \btheta \right)^{-1}\)</span> analyical jacobian is given by <span class="math display">\[
    \frac{\partial \operatorname{U}(\btheta)}{\partial \btheta} = - \sum_{i \in S_A} \frac{\dot{\pi}_i^A\left(\bx_i^{\mathrm{T}} \btheta \right)}{\pi_i^A\left(\bx_i^{\mathrm{T}} \btheta \right)^2} \bx_i \bx_i^{\mathrm{T}}
\]</span> and under <span class="math inline">\(\operatorname{h}\left(\bx_i\right) = \bx_i\)</span> we have</p>
<p><span class="math display">\[
    \frac{\partial \operatorname{U}(\partial \btheta)}{\btheta} = - \sum_{i \in S_B} \frac{\dot{\pi}_i^A\left(\bx_i^{\mathrm{T}} \btheta \right)}{\pi_i^B} \bx_i \bx_i^{\mathrm{T}}.
\]</span></p>
</section>
</section>
<section id="population-mean-estimator-and-its-properties" class="level2" data-number="3.4">
<h2 data-number="3.4" data-anchor-id="population-mean-estimator-and-its-properties"><span class="header-section-number">3.4</span> Population mean estimator and its properties</h2>
<p>With the estimated propensity scores we can consider two approaches for population mean estimation, depending on whether population size is known or not.</p>
<p><span class="math display">\[
\begin{equation*}
    \hat{\mu}_{IPW1} = \frac{1}{N} \sum_{i \in S_A} \frac{y_i}{\hat{\pi}_i^{A}}
\end{equation*}
\]</span></p>
<p><span class="math display">\[
\begin{equation*}
    \hat{\mu}_{IPW2} = \frac{1}{\hat{N}^{A}} \sum_{i \in S_A} \frac{y_i}{\hat{\pi}_i^{A}},
\end{equation*}
\]</span> where <span class="math inline">\(\hat{N^A} = \sum_{i \in S_A} \hat{d}_i^A\)</span>.</p>
<p>In the literature (Kim et al 2020) asymptotic properties of the estimators above are obtained in details. In particular main approach is based on Taylor approximation what is performed in the following subsection.</p>
<section id="variance-of-an-estimator" class="level3" data-number="3.4.1">
<h3 data-number="3.4.1" data-anchor-id="variance-of-an-estimator"><span class="header-section-number">3.4.1</span> Variance of an estimator</h3>
<p>Let <span class="math inline">\(\boldsymbol{\eta} = \left(\mu, \btheta^{T}\right)^{T}\)</span> be the set of parameters to estimate for inverse probability weighting model. The estimator <span class="math inline">\(\hat{\boldsymbol{\eta}} = \left(\hat{\mu}, \hat{\btheta}^{T}\right)^{T}\)</span> is the solution of joint estimating equations <span class="math inline">\(\boldsymbol{\Phi}_n(\boldsymbol{\eta}) = \bZero\)</span>. <span class="math display">\[
\begin{equation}
\boldsymbol{\Phi}_n(\boldsymbol{\eta})=\left(\begin{array}{c}
\frac{1}{N} \sum_{i=1}^N\left[\frac{R_i\left(y_i-\mu\right)}{\pi_i^A}+\Delta \frac{R_i-\pi_i^A}{\pi_i^A}\right] \\
\mathbf{U}(\btheta)
\end{array}\right)=\bZero,
\end{equation}
\]</span> where where <span class="math inline">\(\Delta = \mu\)</span>, if <span class="math inline">\(\hat{\mu} = \hat{\mu}_{IPW1}\)</span> and <span class="math inline">\(\Delta = 0\)</span> if <span class="math inline">\(\hat{\mu} = \hat{\mu}_{IPW2}\)</span>. <span class="math inline">\(\mathbf{U}(\btheta)\)</span> is objective function corresponding to given way od estimation. For example in case od pseudo maximum likelihood estimation, we have gradient corresponding to the choosen link function. In case of GEE it will be on of considered estimating equations. We have <span class="math inline">\(\mathbb{E} \left\{\boldsymbol{\Phi}_n(\boldsymbol{\eta})\right\} = \bZero\)</span> when <span class="math inline">\(\boldsymbol{\eta} = \boldsymbol{\eta}_{0} = \left(\mu_y, \btheta_0^{T}\right)^{T}\)</span>. By applying the first order Taylor expansion around <span class="math inline">\(\boldsymbol{\eta}_0\)</span>, we further have <span class="math display">\[
\begin{equation*}
\hat{\boldsymbol{\eta}}-\boldsymbol{\eta}_0=\left[\boldsymbol{\phi}_n\left(\boldsymbol{\eta}_0\right)\right]^{-1} \boldsymbol{\Phi}_n\left(\boldsymbol{\eta}_0\right)+o_p\left(n_A^{-1 / 2}\right)=\left[E\left\{\boldsymbol{\phi}_n\left(\boldsymbol{\eta}_0\right)\right\}\right]^{-1} \boldsymbol{\Phi}_n\left(\boldsymbol{\eta}_0\right)+o_p\left(n_A^{-1 / 2}\right),
\end{equation*}
\]</span> where <span class="math inline">\(\boldsymbol{\phi}_n(\boldsymbol{\eta})=\partial \boldsymbol{\Phi}_n(\boldsymbol{\eta}) / \partial \boldsymbol{\eta}\)</span>. It follows that <span class="math display">\[
\begin{equation}
\operatorname{Var}(\hat{\boldsymbol{\eta}})=\left[E\left\{\boldsymbol{\phi}_n\left(\boldsymbol{\eta}_0\right)\right\}\right]^{-1} \operatorname{Var}\left\{\boldsymbol{\Phi}_n\left(\boldsymbol{\eta}_0\right)\right\}\left[E\left\{\boldsymbol{\phi}_n\left(\boldsymbol{\eta}_0\right)\right\}^{\top}\right]^{-1}+o\left(n_A^{-1}\right).
\end{equation}
\]</span> Let us show how to derive this variance-covariance matrix in case of MLE with logistic regression model. Calculations for the rest of the models is available in Appendix and you are welcome to take a look at them. Thus, we have <span class="math display">\[
\boldsymbol{\Phi}_n(\boldsymbol{\eta})=\left(\begin{array}{c}
\frac{1}{N} \sum_{i=1}^N\left[\frac{R_i\left(y_i-\mu\right)}{\pi_i^A}+\Delta \frac{R_i-\pi_i^A}{\pi_i^A}\right] \\
\frac{1}{N} \sum_{i=1}^N R_i \boldsymbol{x}_i-\frac{1}{N} \sum_{i \in \mathcal{S}_B} d_i^B \pi_i^A \boldsymbol{x}_i
\end{array}\right)
\]</span> and</p>
<p><span class="math display">\[
\phi_n(\boldsymbol{\eta})=\frac{1}{N}\left(\begin{array}{cc}
-\sum_{i=1}^N\left\{\left(1-\frac{\Delta}{\mu}\right) \frac{R_i}{\pi_i^A}+\frac{\Delta}{\mu}\right\} &amp; -\sum_{i=1}^N \frac{1-\pi_i^A}{\pi_i^A} R_i\left(y_i-\mu+\Delta\right) \boldsymbol{x}_i^{\top} \\
\mathbf{0} &amp; -\sum_{i \in \mathcal{S}_B} d_i^B \pi_i^A\left(1-\pi_i^A\right) \boldsymbol{x}_i \boldsymbol{x}_i^{\top}
\end{array}\right)
\]</span> It can be shown that <span class="math display">\[
\left[E\left\{\boldsymbol{\phi}_n(\boldsymbol{\eta})\right\}\right]^{-1}=\left(\begin{array}{cc}
-1 &amp;  \mathbf{b}^{\top} \\
\mathbf{0} &amp; -\left[\frac{1}{N} \sum_{i=1}^N \pi_i^A\left(1-\pi_i^A\right) \boldsymbol{x}_i \boldsymbol{x}_i^{\top}\right]^{-1}
\end{array}\right)
\]</span> where</p>
<p><span class="math display">\[
\mathbf{b}^{\top} =
\left\{N^{-1} \sum_{i=1}^N\left(1-\pi_i^{\mathrm{A}}\right) \left(y_i-\mu_y + \Delta\right) x_i^{\top}\right\}\left\{N^{-1} \sum_{i=1}^N \pi_i^{\mathrm{A}}\left(1-\pi_i^{\mathrm{A}}\right) \boldsymbol{x}_i x_i^{\top}\right\}^{-1}
\]</span></p>
<p><span class="math inline">\(\operatorname{Var}\left\{\boldsymbol{\Phi}_n\left(\boldsymbol{\eta}_0\right)\right\}\)</span> can be found using decomposition of <span class="math inline">\(\bPhi_n(\boldsymbol{\eta}) = \bA_1 + \bA_2\)</span>. Then <span class="math inline">\(\operatorname{Var}\left\{\boldsymbol{\Phi}_n\left(\boldsymbol{\eta}_0\right)\right\} = \operatorname{Var}\left(\bA_1\right) + \operatorname{Var}\left(\bA_2\right)\)</span>. Let <span class="math display">\[
\mathbf{A}_1=\frac{1}{N} \sum_{i=1}^N\left(\begin{array}{c}
\frac{R_i\left(y_i-\mu\right)}{\pi_i^A}+\Delta \frac{R_i-\pi_i^A}{\pi_i^A} \\
R_i \boldsymbol{x}_i-\pi_i^A \boldsymbol{x}_i
\end{array}\right), \quad \mathbf{A}_2=\frac{1}{N}\left(\begin{array}{c}
0 \\
\sum_{i=1}^N \pi_i^A \boldsymbol{x}_i-\sum_{i \in S_B} d_i^B \pi_i^A \boldsymbol{x}_i
\end{array}\right)
\]</span></p>
<p>With this division, we have <span class="math inline">\(\operatorname{Var}\left\{\boldsymbol{\Phi}_n\left(\boldsymbol{\eta}_0\right)\right\}=\mathbf{V}_1+\mathbf{V}_2\)</span> where <span class="math inline">\(\mathbf{V}_1 = Var \left(A_1\right)\)</span> and <span class="math inline">\(\mathbf{V}_2 = Var \left(A_2\right)\)</span>. <span class="math inline">\(V_1\)</span> depends only on the model for propensity score and <span class="math inline">\(V_2\)</span> on sampling design for probability sample, both evaluated on <span class="math inline">\(\boldsymbol{\eta} = \boldsymbol{\eta}_0\)</span>. Finally we have <span class="math display">\[
\mathbf{V}_1=\frac{1}{N^2} \sum_{i=1}^N\left(\begin{array}{cc}
\left\{\left(1-\pi_i^A\right) / \pi_i^A\right\}\left(y_i-\mu+\Delta\right)^2 &amp; \left(1-\pi_i^A\right)\left(y_i-\mu+\Delta\right) \boldsymbol{x}_i^{\top} \\
\left(1-\pi_i^A\right)\left(y_i-\mu+\Delta\right) \boldsymbol{x}_i &amp; \pi_i^A\left(1-\pi_i^A\right) \boldsymbol{x}_i \boldsymbol{x}_i^{\top}
\end{array}\right)
\]</span> and <span class="math display">\[
\mathbf{V}_2=\left(\begin{array}{ll}
0 &amp; \mathbf{0}^{\top} \\
\mathbf{0} &amp; \mathbf{D}
\end{array}\right)
\]</span> where <span class="math inline">\(\mathbf{D}=N^{-2} V_p\left(\sum_{i \in \mathcal{S}_B} d_i^B \pi_i^A \boldsymbol{x}_i\right)\)</span> and is given by <span class="math display">\[
\begin{equation}
{\mathbf{D}}=\frac{1}{N^2} \sum_{i \in \mathcal{S}_{\mathrm{B}}} \sum_{j \in \mathcal{S}_{\mathrm{B}}} \frac{\pi_{i j}^{\mathrm{B}}-\pi_i^{\mathrm{B}} \pi_j^{\mathrm{B}}}{\pi_{i j}^{\mathrm{B}}} \frac{{\pi}_i^{\mathrm{A}}}{\pi_i^{\mathrm{B}}} \frac{{\pi}_j^{\mathrm{A}}}{\pi_j^{\mathrm{B}}} \boldsymbol{x}_i \boldsymbol{x}_j^{\top}
\end{equation}
\]</span> The asymptotic variance for <span class="math inline">\(\mu_{IPW}\)</span> is the first diagonal element of matrix <span class="math display">\[
\left[E\left\{\boldsymbol{\phi}_n\left(\boldsymbol{\eta}_0\right)\right\}\right]^{-1} \operatorname{Var}\left\{\boldsymbol{\Phi}_n\left(\boldsymbol{\eta}_0\right)\right\}\left[E\left\{\boldsymbol{\phi}_n\left(\boldsymbol{\eta}_0\right)\right\}^{\top}\right]^{-1}.
\]</span></p>


</section>
</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    target: function(trigger) {
      return trigger.previousElementSibling;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    setTimeout(function() {
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  function tippyHover(el, contentFn) {
    const config = {
      allowHTML: true,
      content: contentFn,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start'
    };
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const cites = ref.parentNode.getAttribute('data-cites').split(' ');
    tippyHover(ref, function() {
      var popup = window.document.createElement('div');
      cites.forEach(function(cite) {
        var citeDiv = window.document.createElement('div');
        citeDiv.classList.add('hanging-indent');
        citeDiv.classList.add('csl-entry');
        var biblioDiv = window.document.getElementById('ref-' + cite);
        if (biblioDiv) {
          citeDiv.innerHTML = biblioDiv.innerHTML;
        }
        popup.appendChild(citeDiv);
      });
      return popup.innerHTML;
    });
  }
});
</script>
<nav class="page-navigation">
  <div class="nav-page nav-page-previous">
      <a href="./intro.html" class="pagination-link">
        <i class="bi bi-arrow-left-short"></i> <span class="nav-page-text"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">Introduction and Overview</span></span>
      </a>          
  </div>
  <div class="nav-page nav-page-next">
      <a href="./mi.html" class="pagination-link">
        <span class="nav-page-text"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">Mass imputation</span></span> <i class="bi bi-arrow-right-short"></i>
      </a>
  </div>
</nav>
</div> <!-- /content -->



</body></html>